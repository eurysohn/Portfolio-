Lecture 1: Characteristics and Examples of Time Series Data Introduction to Time Series, Fall 2023 Ryan Tibshirani Related reading: Chapters 1.1–1.2 of Shumway and Stoﬀer (SS); Chapters 2.3 and 3.2–3.6 of Hyndman and Athanasopoulos (HA). 1 Course stuﬀ • Instructor: Ryan Tibshirani • GSI: Alice Cima • Reader: David Zhang • Course website: https://www.stat.berkeley.edu/~ryantibs/timeseries-f23/ • Everything will be up on the website: lecture notes, homework assignments, syllabus, schedule, links to bCourses, Ed discussion, etc. • Please email the GSI with any issues ﬁrst. The Instructor will be looped in only as-needed • Please call me Ryan, Professor Tibshirani, or Professor Tibs. Please DO NOT call me Professor • There will be 5 homework assignments, 1 midterm, and 1 ﬁnal exam. Syllabus gives details on grad- ing breakdown • The homeworks will be due about every 2 weeks, with spacing for the midterm and the ﬁnal. Sched- ule on website gives projected dates • Probability at the level of Stat 134 or Data 140 is required as a pre-req. Statistics at the level of Stat 133 and 135 is recommend and may be taken concurrently • We will also assume basic level of ﬂuency in R programming. You will need to have R installed, and it will be very helpful for you to have RStudio installed • Read the course website or the syllabus for the projected list of topics that we will cover • Read the syllabus for late policy for homeworks, and collaboration policy • Do not copy or cheat. It will not end well and dealing with it is really not fun for anyone involved • Ok, now on to the fun stuﬀ! 2 Time series intro • What ﬁelds do time series data occur in? Economics, social science, epidemiology, medicine, neuro- science, language modeling, ... – Economics: stock prices or stock returns over time – Social science: birth rates or school acceptance rates over time – Epidemiology: Covid-19 cases or Inﬂuenza hospitalizations over time – Medicine: blood antibody levels over time (IgA, IgG, IgM, ...) – Neuroscience: brain-wave patterns over time, under diﬀerent conditions 1

– Language modeling: word or token distributions over time • What distinguishes time series from traditional (batch) data problems? The data are not i.i.d. (independent and identically distributed). There is correlation in- duced by the fact that we are making observations over time. • Ignoring these correlations is going to be problematic. Enter time series analysis, models, and fore- casts • Worth mentioning at the outset that there are two view in classical time series analysis: time domain and frequency domain (also called spectral ) approaches. – Time domain: language/tools for studying lagged relationships—e.g., what happened yesterday will inﬂuence today and tomorrow – Frequency domain: language/tools for studying study seasonality and cycles • These are not mutually exclusive. We will mostly focus on the former (time domain approaches), but will brieﬂy introduce the latter (frequency domain approaches) a bit later in the course • We will also use a signiﬁcant chunk of the course to emphasize the predictive perspective: forecast- ing, practical considerations therein, and important related topics like calibration and ensembling 3 Time series examples • We’ll step through the following examples, in Figures 1–7, and discuss each, including why the data cannot really be i.i.d. Figure 1: Johnson & Johnson quarterly earnings per share (from SS). 2

Figure 2: Yearly average global temperature deviations from the 1951–1980 average (from SS). Figure 3: Vocal response data measured from the syllable “aaa · · · hhh” (from SS). 3

Figure 4: Blood oxygenation-level dependent (BOLD) signal intensity in regions of the cortex (from SS). Figure 5: BOLD signal intensity in regions of the thalamus and cerebellum (from SS). 4

Figure 6: Reported Covid-19 cases per 100k people in 6 large US states. Figure 7: Reported Covid-19 deaths per 100k people in 6 large US states. 5

4 White noise • The term white noise is used a lot in time series in related ﬁelds. It simply refers to a sequence xt, t = 1, 2, 3, . . . of uncorrelated random variables, with zero mean, and constant variance. Precisely, Cov(xs, xt) = 0, for all s (cid:54)= t E(xt) = 0, Var(xt) = σ2, for all t • (Recall ... for random variables x, y, their covariance is (cid:104) Cov(x, y) = E (cid:105) (x − E[x])(y − E[y]) and Cov(x, x) = Var(x). The correlation between x, y is Cor(x, y) = Cov(x, y) (cid:112)Var(x)(cid:112)Var(y) Therefore zero correlation and zero covariance are equivalent properties) • A stronger property than white noise is i.i.d. white noise, that is, a sequence of white noise whose elements are also i.i.d. • Why is this stronger? First, the distributions of the elements in a white noise sequence do not need to be the same—they only need to have the same ﬁrst two moments (mean and variance). Second, white noise requires only zero correlation, not independence • (Can you give an example of uncorrelated but not independent random variables?) • An even stronger property is Gaussian white noise, that is, a sequence of white noise whose elements are also jointly Gaussian distributed • Why is this stronger than i.i.d. white noise? Because if two Gaussians have equal mean and variance, then they are the same distribution; and, for Gaussians, zero correlation implies independence • To summarize, {Gaussian white noise sequences} ⊆ {i.i.d. white noise sequences} ⊆ {white noise sequences} • A Gaussian white noise sequence is plotted below, in Figure 8. Do any of the time series examples above look like white noise? No. White noise is not a great model for time series data, which typi- cally has both trends (nonconstant mean and variance) and dependence (nonzero correlation). But it is an important concept and will serve as a building block for more complex models 5 Linear ﬁltering • Another important concept in time series is ﬁltering. ﬁelds. A linear ﬁlter is just the result of per- forming a moving linear combination of a series xt, t = 1, 2, 3, . . . , with given weights. (Nonlinear ﬁlters take nonlinear combinations and we won’t talk about them) • The simplest and most common type of linear ﬁlter is a moving average. For example, a moving average, that is centered around lag 0, of window length 3, is yt = (cid:16) 1 3 xt−1 + xt + xt+1 (cid:17) In principle, we could center the moving average wherever we want. But the term moving average (without further speciﬁcation) usually means that we center it at lag 0 6

Figure 8: Gaussian white noise. • When we center the moving average so that its right endpoint is at time t, ensuring we only average past values, this is called a trailing average. For example, a trailing average of length 3 is yt = (cid:16) 1 3 xt−2 + xt−1 + xt (cid:17) The Covid-19 data plotted above (Figures 6 and 7 was actually ﬁltered with a 7-day trailing average) • A general linear ﬁlter takes the form ∞ (cid:88) yt = aixt−i i=−∞ for constants ai, where typically only ﬁnitely many are nonzero. For example, the second to last example took a−1 = a0 = a1 = 1/3, and the last example took a0 = a1 = a2 = 1/3 • Linear ﬁlters provide a form of smoothing for time series, which we’ll revisit brieﬂy later in the lec- ture, and then in more detail in a future week 6 Autoregression • An autoregressive process is one that takes the form xt = k (cid:88) i=1 βixt−i + (cid:15)t for coeﬃcients β1, . . . , βk and errors (cid:15)t • Typically we assume that the errors are a white noise sequence 7

• The value of k is called the order of the autoregressive process, and the abbreviation AR(k) is com- mon. So, for example, AR(3) means an autoregressive process with lag 3: each value in the sequence depends on the last 3 values • The simplest autoregressive process is AR(1), with coeﬃcient β1 = 1: this is also known as a random walk xt = xt−1 + (cid:15)t • Random walks may seem very simple and trivial at ﬁrst but actually they and simple generalizations are pretty fascinating, and important • For example, did you know that a random walk in 1 and 2 dimensions is recurrent (returns to where it started—say, the origin—inﬁnitely often with probability 1), but in 3 dimensions and higher it is transient (returns to the origin inﬁnitely often with probability 0) • (And did you know that Larry Brown proved in 19711 that this last fact is equivalent in some precise sense to Stein’s paradox: that the MLE in a normal means model is admissible in dimensions 1 and 2, and inadmissible in dimensions 3 and higher??) • You could also say that random walks were the beginning of what made Google the giant they are today (the “billion dollar eigenvector”) • Ok, back to the main story, a random walk with drift takes the form for some δ > 0. Figure 9 plots examples of random walks with and without drift • By unraveling the last iteration, we can write a random walk with drift equivalently as (assuming we xt = δ + xt−1 + (cid:15)t start at x0 = 0): i=1 Think: what happens to the variance of this as t grows? xt = δt + t (cid:88) (cid:15)i 7 Signal plus noise • A useful general time series model is called the signal plus noise model, of the form xt = θt + (cid:15)t where the errors et, t = 1, 2, 3, . . . may be white noise or may be correlated over time • The problem of estimating the signal θt, t = 1, 2, 3, . . . is of great interest in many applications • It is common in time series to think about decompositions for the signal sequence, into a trend ut and seasonal components st: θt = ut + st • The seasonal component st has a regular/periodic behavior for some ﬁxed period. For example: – Pediatric doctor’s oﬃce visits dip on weekends (weekly period) – Gambling goes up at the beginning of each month (monthly period) – Chocolate purchases go up on and around Valentine’s day (yearly period) 1Larry Brown (1971), “Admissible Estimators, Recurrent Diﬀusions, and Insoluble Boundary Value Problems” 8

Figure 9: Random walk without and with drift. • The trend component ut is not regular, and is typically not assumed to be linear or to have any par- ticular parametric form; it is typically estimated nonparametrically using some kind of smoother— more on this later in the course • (Some authors even further decompose the trend into two components: proper trend and cycle. The former is monotone and the latter has a cyclic behavior but without a ﬁxed period. We don’t gener- ally ﬁnd this a useful distinction and won’t really pursue this ... but it may be good to know about in case you hear people, particularly economists, mentioning: trend, seasonal, and cyclic components separately) • Economists and oﬃcial statistics agencies (like the US Census Bureau) care a lot about decompo- sitions into trend and seasonal components ... there are various methods for doing so that we may cover later in the course: what is considered the “classical” decomposition, but also X-11 (developed by the US Census Bureau and Statistics Canada), SEATS (developed by the Bank of Spain), and STL (developed by academics at the University of Michigan and Bell Labs) • Many consider STL to be the most general and robust method for decomposition. Figure 10 gives an example applied to US retail employment data 9

Figure 10: STL decomposition of US retail employment data (from HA). 10